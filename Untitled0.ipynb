{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FfMvJxI3f9jR"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer:\n",
        "    def __init__(self, input_size, output_size, activation):\n",
        "        self.weights = np.random.randn(input_size, output_size) * 0.01\n",
        "        self.biases = np.zeros(output_size)\n",
        "        self.activation = activation"
      ],
      "metadata": {
        "id": "udWZCdO2er0L"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "    def forward(self, inputs):\n",
        "        self.inputs = inputs\n",
        "        self.z = np.dot(inputs, self.weights) + self.biases\n",
        "        self.output = self.activation_function(self.z)\n",
        "        return self.output\n",
        "\n",
        "    def activation_function(self, x):\n",
        "        if self.activation == 'sigmoid':\n",
        "            return 1 / (1 + np.exp(-x))\n",
        "        elif self.activation == 'relu':\n",
        "            return np.maximum(0, x)\n",
        "        else:\n",
        "            return x  # Linear activation\n",
        "\n",
        "    def activation_derivative(self, x):\n",
        "        if self.activation == 'sigmoid':\n",
        "            return x * (1 - x)\n",
        "        elif self.activation == 'relu':\n",
        "            return np.where(x > 0, 1, 0)\n",
        "        else:\n",
        "            return 1  # Linear activation"
      ],
      "metadata": {
        "id": "DiBCMh55exj2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " def backward(self, output_error, learning_rate):\n",
        "        # Calculate error for the current layer\n",
        "        delta = output_error * self.activation_derivative(self.output)\n",
        "\n",
        "        # Calculate error for the previous layer\n",
        "        input_error = np.dot(delta, self.weights.T)\n",
        "\n",
        "        # Update weights and biases\n",
        "        self.weights -= learning_rate * np.dot(self.inputs.T, delta)\n",
        "        self.biases -= learning_rate * np.sum(delta, axis=0)\n",
        "\n",
        "        return input_error"
      ],
      "metadata": {
        "id": "En7NWuUXe7AM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self):\n",
        "        self.layers = []\n",
        "\n",
        "    def add_layer(self, layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    def predict(self, inputs):\n",
        "        output = inputs\n",
        "        for layer in self.layers:\n",
        "            output = layer.forward(output)\n",
        "        return output\n",
        "\n",
        "    def train(self, inputs, targets, epochs, learning_rate):\n",
        "        for epoch in range(epochs):\n",
        "            output = self.predict(inputs)\n",
        "            error = targets - output\n",
        "\n",
        "            # Backpropagate the error through the layers\n",
        "            layer_error = error\n",
        "            for layer in reversed(self.layers):\n",
        "                layer_error = layer.backward(layer_error, learning_rate)\n",
        "\n",
        "            # Print the loss for the current epoch (optional)\n",
        "            loss = np.mean(np.square(error))\n",
        "            print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss}')"
      ],
      "metadata": {
        "id": "KNcEBST-fHzd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a neural network\n",
        "network = NeuralNetwork()\n",
        "network.add_layer(Layer(2, 4, 'sigmoid'))  # Input layer with 2 neurons, hidden layer with 4 neurons, sigmoid activation\n",
        "network.add_layer(Layer(4, 1, 'linear'))  # Hidden layer with 4 neurons, output layer with 1 neuron, linear activation\n"
      ],
      "metadata": {
        "id": "8cAke6owfN75"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training data\n",
        "inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "targets = np.array([[0], [1], [1], [0]])"
      ],
      "metadata": {
        "id": "R240d9t_faKx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer:\n",
        "    def __init__(self, input_size, output_size, activation):\n",
        "        self.weights = np.random.randn(input_size, output_size) * 0.01\n",
        "        self.biases = np.zeros(output_size)\n",
        "        self.activation = activation\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        self.inputs = inputs\n",
        "        self.z = np.dot(inputs, self.weights) + self.biases\n",
        "        self.output = self.activation_function(self.z)\n",
        "        return self.output\n",
        "\n",
        "    def activation_function(self, x):\n",
        "        if self.activation == 'sigmoid':\n",
        "            return 1 / (1 + np.exp(-x))\n",
        "        elif self.activation == 'relu':\n",
        "            return np.maximum(0, x)\n",
        "        else:\n",
        "            return x  # Linear activation\n",
        "\n",
        "    def activation_derivative(self, x):\n",
        "        if self.activation == 'sigmoid':\n",
        "            return x * (1 - x)\n",
        "        elif self.activation == 'relu':\n",
        "            return np.where(x > 0, 1, 0)\n",
        "        else:\n",
        "            return 1  # Linear activation\n",
        "\n",
        "    def backward(self, output_error, learning_rate):\n",
        "        # Calculate error for the current layer\n",
        "        delta = output_error * self.activation_derivative(self.output)\n",
        "\n",
        "        # Calculate error for the previous layer\n",
        "        input_error = np.dot(delta, self.weights.T)\n",
        "\n",
        "        # Update weights and biases\n",
        "        self.weights -= learning_rate * np.dot(self.inputs.T, delta)\n",
        "        self.biases -= learning_rate * np.sum(delta, axis=0)\n",
        "\n",
        "        return input_error"
      ],
      "metadata": {
        "id": "pZRtcPU5gYIY"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the network\n",
        "network.train(inputs, targets, epochs=10000, learning_rate=0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "T5KPlFgQgabQ",
        "outputId": "c109f1a8-415c-459e-af8c-6b71cc7ed3cc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'Layer' object has no attribute 'forward'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-b5d45077108f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-d999fb85d397>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, inputs, targets, epochs, learning_rate)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-d999fb85d397>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Layer' object has no attribute 'forward'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "predictions = network.predict(inputs)\n",
        "print(predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "t6IBgWY2gitu",
        "outputId": "c5401646-a2a0-4ad0-8a81-b17137c84e15"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'Layer' object has no attribute 'forward'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-25e3110c597b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Make predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-d999fb85d397>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Layer' object has no attribute 'forward'"
          ]
        }
      ]
    }
  ]
}
# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17G12eDBrpjH4hoTmEbuE2oRLgMoM-l3f
"""

import numpy as np

class Layer:
    def __init__(self, input_size, output_size, activation):
        self.weights = np.random.randn(input_size, output_size) * 0.01
        self.biases = np.zeros(output_size)
        self.activation = activation

def forward(self, inputs):
        self.inputs = inputs
        self.z = np.dot(inputs, self.weights) + self.biases
        self.output = self.activation_function(self.z)
        return self.output

    def activation_function(self, x):
        if self.activation == 'sigmoid':
            return 1 / (1 + np.exp(-x))
        elif self.activation == 'relu':
            return np.maximum(0, x)
        else:
            return x  # Linear activation

    def activation_derivative(self, x):
        if self.activation == 'sigmoid':
            return x * (1 - x)
        elif self.activation == 'relu':
            return np.where(x > 0, 1, 0)
        else:
            return 1  # Linear activation

def backward(self, output_error, learning_rate):
        # Calculate error for the current layer
        delta = output_error * self.activation_derivative(self.output)

        # Calculate error for the previous layer
        input_error = np.dot(delta, self.weights.T)

        # Update weights and biases
        self.weights -= learning_rate * np.dot(self.inputs.T, delta)
        self.biases -= learning_rate * np.sum(delta, axis=0)

        return input_error

class NeuralNetwork:
    def __init__(self):
        self.layers = []

    def add_layer(self, layer):
        self.layers.append(layer)

    def predict(self, inputs):
        output = inputs
        for layer in self.layers:
            output = layer.forward(output)
        return output

    def train(self, inputs, targets, epochs, learning_rate):
        for epoch in range(epochs):
            output = self.predict(inputs)
            error = targets - output

            # Backpropagate the error through the layers
            layer_error = error
            for layer in reversed(self.layers):
                layer_error = layer.backward(layer_error, learning_rate)

            # Print the loss for the current epoch (optional)
            loss = np.mean(np.square(error))
            print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss}')

# Create a neural network
network = NeuralNetwork()
network.add_layer(Layer(2, 4, 'sigmoid'))  # Input layer with 2 neurons, hidden layer with 4 neurons, sigmoid activation
network.add_layer(Layer(4, 1, 'linear'))  # Hidden layer with 4 neurons, output layer with 1 neuron, linear activation

# Training data
inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
targets = np.array([[0], [1], [1], [0]])

class Layer:
    def __init__(self, input_size, output_size, activation):
        self.weights = np.random.randn(input_size, output_size) * 0.01
        self.biases = np.zeros(output_size)
        self.activation = activation

    def forward(self, inputs):
        self.inputs = inputs
        self.z = np.dot(inputs, self.weights) + self.biases
        self.output = self.activation_function(self.z)
        return self.output

    def activation_function(self, x):
        if self.activation == 'sigmoid':
            return 1 / (1 + np.exp(-x))
        elif self.activation == 'relu':
            return np.maximum(0, x)
        else:
            return x  # Linear activation

    def activation_derivative(self, x):
        if self.activation == 'sigmoid':
            return x * (1 - x)
        elif self.activation == 'relu':
            return np.where(x > 0, 1, 0)
        else:
            return 1  # Linear activation

    def backward(self, output_error, learning_rate):
        # Calculate error for the current layer
        delta = output_error * self.activation_derivative(self.output)

        # Calculate error for the previous layer
        input_error = np.dot(delta, self.weights.T)

        # Update weights and biases
        self.weights -= learning_rate * np.dot(self.inputs.T, delta)
        self.biases -= learning_rate * np.sum(delta, axis=0)

        return input_error

# Train the network
network.train(inputs, targets, epochs=10000, learning_rate=0.1)

# Make predictions
predictions = network.predict(inputs)
print(predictions)